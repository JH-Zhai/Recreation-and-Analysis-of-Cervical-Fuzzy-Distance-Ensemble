{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.7.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fQ0Am7XPsHEU",
        "outputId": "fa0b4a1a-5ef3-4e7c-cdec-22319f1b3c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.4\n",
            "  Downloading tensorflow-2.7.4-cp39-cp39-manylinux2010_x86_64.whl (496.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.1/496.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (16.0.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (1.22.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (3.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (1.4.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (0.32.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (2.12.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (0.40.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (1.16.0)\n",
            "Collecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (4.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (1.53.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (2.2.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (3.3.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.4) (0.4.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (67.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (2.17.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (0.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.4) (1.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.4) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.4) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.4) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow==2.7.4) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.4) (6.3.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.4) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.4) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.4) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.4) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7.4) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.4) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.4) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow==2.7.4) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, flatbuffers, protobuf, keras-preprocessing, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed flatbuffers-2.0.7 keras-2.7.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorflow-2.7.4 tensorflow-estimator-2.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flatbuffers",
                  "google",
                  "keras",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eVWyg3X0-8og"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import scipy\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "import argparse\n",
        "from torch import optim, cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Whether to train on a gpu\n",
        "train_on_gpu = cuda.is_available()\n",
        "print(f'Train on gpu: {train_on_gpu}')\n",
        "multi_gpu = False\n",
        "# Number of gpus\n",
        "if train_on_gpu:\n",
        "    gpu_count = cuda.device_count()\n",
        "    print(f'{gpu_count} gpus detected.')\n",
        "    if gpu_count > 1:\n",
        "        multi_gpu = True\n",
        "    else:\n",
        "        multi_gpu = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwomsZ3dtsHm",
        "outputId": "72341786-c356-4acd-d85c-998c741f4181"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train on gpu: True\n",
            "1 gpus detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_csv(path):\n",
        "    print(\"CSV being generated\")\n",
        "    uniques = [\"Dyskeratotic\", \"Koilocytotic\", \"Metaplastic\", \"Parabasal\", \"SuperficialIntermediate\"]\n",
        "    dirs = [\"train\", \"test\"]\n",
        "\n",
        "    \"\"\"\n",
        "            +-- train\n",
        "            |   +-- Dyskeratotic\n",
        "            |   +-- Koilocytotic\n",
        "            |   +-- Metaplastic\n",
        "            |   +-- Parabasal\n",
        "            |   +-- SuperficialIntermediate\n",
        "\n",
        "            +-- test\n",
        "            |   +-- Dyskeratotic\n",
        "            |   +-- Koilocytotic\n",
        "            |   +-- Metaplastic\n",
        "            |   +-- Parabasal\n",
        "            |   +-- SuperficialIntermediate\n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "    # Above is the expected directory structure\n",
        "\n",
        "    data = []\n",
        "    for d in dirs:\n",
        "        for unique in uniques:\n",
        "            directory = path + \"/\" + d + \"/\" + unique  # required path\n",
        "\n",
        "            for filename in os.listdir(directory):\n",
        "                paths = directory + \"/\" + filename  # required path\n",
        "                data.append([filename, paths, unique])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"filename\", \"path\", \"class\"])\n",
        "    df = shuffle(df)\n",
        "    name = \"/content/drive/MyDrive/Colab Notebooks/CSC413/final/csv_files/\" + \"Data-full\"  # required path\n",
        "    df.to_csv(name, index=False)\n",
        "    print(\"Generation Complete\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "P4034Mp1vOPM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_splits(x, y, files_for_train_x, files_for_validation_x, files_for_train_y, files_for_validation_y,\n",
        "                  n_splits=5):\n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "    # fold_no = 1\n",
        "    for train_index, val_index in kf.split(x):\n",
        "        # it will split the entire data into 5 folds\n",
        "        x_train, x_val = x[train_index], x[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # split the into 5 folds\n",
        "\n",
        "        files_for_train_x.append(x_train)\n",
        "        files_for_validation_x.append(x_val)\n",
        "        files_for_train_y.append(y_train)\n",
        "        files_for_validation_y.append(y_val)\n",
        "        # fold_no += 1"
      ],
      "metadata": {
        "id": "ncGpYyK1vPSw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_name, IMG_SIZE=256, output=5):\n",
        "    IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)  # IMG_SIZE = 256\n",
        "    if (model_name == \"MobileNetV2\"):\n",
        "\n",
        "        model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                                  include_top=False,\n",
        "                                                  weights='imagenet')\n",
        "    elif (model_name == \"InceptionV3\"):\n",
        "        model = tf.keras.applications.inception_v3.InceptionV3(input_shape=IMG_SHAPE,\n",
        "                                                               include_top=False,\n",
        "                                                               weights='imagenet')\n",
        "\n",
        "    elif (model_name == \"InceptionResNetV2\"):\n",
        "        model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=IMG_SHAPE,\n",
        "                                                                            include_top=False,\n",
        "                                                                            weights='imagenet')\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(model.output)\n",
        "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(output, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=model.input, outputs=x)\n",
        "\n",
        "    my_model = tf.keras.models.clone_model(model)\n",
        "    return my_model"
      ],
      "metadata": {
        "id": "qo40dS7XvWU-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fuzzy_dist(classifier1, classifier2, classifier3, verbose=True):\n",
        "    out = np.empty(len(classifier1))\n",
        "    for i in range(len(classifier1)):\n",
        "        if np.argmax(classifier1[i]) == np.argmax(classifier2[i]) == np.argmax(classifier3[i]):\n",
        "            out[i] = np.argmax(classifier2[i])\n",
        "        else:\n",
        "            measure = np.zeros(len(classifier1[i]))\n",
        "            for j in range(len(classifier1[i])):\n",
        "                scores = np.array(\n",
        "                    [classifier1[i, j], classifier2[i, j], classifier3[i, j]])\n",
        "                measure[j] = scipy.spatial.distance.cosine(np.ones(3), scores) * scipy.spatial.distance.euclidean(\n",
        "                    np.ones(3), scores) * scipy.spatial.distance.cityblock(np.ones(3), scores)\n",
        "                if verbose:\n",
        "                    print(measure)\n",
        "            out[i] = np.argmin(measure)\n",
        "    return out"
      ],
      "metadata": {
        "id": "oC0lQsJWvx8F"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Dyskeratotic\" , \"Koilocytotic\" , \"Metaplastic\" , \"Parabasal\" , \"SuperficialIntermediate\"\n",
        "# this function will encode the labels\n",
        "# for five classes\n",
        "def encode_y(y):\n",
        "    Y = []\n",
        "    for i in y:\n",
        "        if i == \"Dyskeratotic\":\n",
        "            Y.append(0)\n",
        "        elif i == \"Koilocytotic\":\n",
        "            Y.append(1)\n",
        "        if i == \"Metaplastic\":\n",
        "            Y.append(2)\n",
        "        if i == \"Parabasal\":\n",
        "            Y.append(3)\n",
        "        if i == \"SuperficialIntermediate\":\n",
        "            Y.append(4)\n",
        "\n",
        "    return np.array(Y).astype(\"float32\")\n",
        "\n",
        "\n",
        "# convert file paths info nums \n",
        "# then normalize\n",
        "def process_x(x):\n",
        "    lis = []\n",
        "    for i in x:\n",
        "        img = cv2.imread(i)\n",
        "        resize = cv2.resize(img, (256, 256))\n",
        "        lis.append(resize)\n",
        "    return np.array(lis).astype(\"float32\") / 255.0\n",
        "\n",
        "\n",
        "\n",
        "def k_fold_separate(x_train, y_train, x_val, y_val, model_name1, model_name2, model_name3, fold_no, NUM_EPOCHS=70,\n",
        "                    train_batch=16, validation_batch=16, lr=1e-4):\n",
        "    train_datagen = ImageDataGenerator(rotation_range=40,\n",
        "                                       width_shift_range=0.2,\n",
        "                                       height_shift_range=0.2,\n",
        "                                       shear_range=0.2,\n",
        "                                       zoom_range=0.2,\n",
        "                                       horizontal_flip=True)\n",
        "\n",
        "    val_datagen = ImageDataGenerator()\n",
        "\n",
        "    x_train, y_train, x_val, y_val = process_x(x_train), encode_y(y_train), process_x(x_val), encode_y(y_val)\n",
        "\n",
        "    train = train_datagen.flow(x_train, y_train, batch_size=train_batch)\n",
        "    validation = val_datagen.flow(x_val, y_val,\n",
        "                                  batch_size=validation_batch)\n",
        "    test = x_val\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print()\n",
        "    print(\"fold no --- \", fold_no)\n",
        "    print()\n",
        "    print('------------------------------------------------------------------------')\n",
        "\n",
        "    y_preds = []\n",
        "    print()\n",
        "    print(model_name1)\n",
        "    print()\n",
        "\n",
        "    model1 = create_model(model_name1)\n",
        "\n",
        "    # Compile the model\n",
        "    model1.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, decay=0.0001),\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "    # Generate a print\n",
        "\n",
        "    # Fit data to model\n",
        "    history1 = model1.fit(x=train,\n",
        "                          validation_data=validation,\n",
        "                          epochs=NUM_EPOCHS\n",
        "\n",
        "                          )\n",
        "\n",
        "    # model save..\n",
        "    model_saved_name = model_name1 + \"_weights\"+ \"_\" + str(fold_no) + \".h5\"\n",
        "\n",
        "    model1.save_weights(\"require path\" + model_saved_name)\n",
        "\n",
        "    hist_df = pd.DataFrame(history1.history) \n",
        "    hist_csv_file =  \"history_\" + model_name1 + \"_weights\" + \"_\" + str(fold_no) + \".csv\"\n",
        "    filepath = \"require path\" + hist_csv_file \n",
        "    with open(filepath, mode='w') as f:\n",
        "        hist_df.to_csv(f)\n",
        "\n",
        "    print(f'{model_saved_name} saved')\n",
        "    print(f'{hist_csv_file} saved')\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores = model1.evaluate(validation)\n",
        "    print(\n",
        "        f'Score for fold {fold_no}: {model1.metrics_names[0]} of {scores[0]}; {model1.metrics_names[1]} of {scores[1] * 100}%')\n",
        "    # predictions = model.predict()\n",
        "    preds1 = model1.predict(test, batch_size=validation_batch)\n",
        "    for pred in preds1:\n",
        "        y_preds.append(np.argmax(pred))\n",
        "    print('Accuracy Score: ', accuracy_score(y_val, y_preds))\n",
        "    n = len(precision_score(y_val, y_preds, average=None))\n",
        "    print('Precision Score(Class wise): ', precision_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(precision_score(y_val, y_preds, average=None)) / n)\n",
        "    print('Recall Score(Class wise): ', recall_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(recall_score(y_val, y_preds, average=None)) / n)\n",
        "    print('F1 Score(Class wise): ', f1_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(f1_score(y_val, y_preds, average=None)) / n)\n",
        "    print('Conf Matrix Score(Class wise):\\n ', confusion_matrix(y_val, y_preds))\n",
        "\n",
        "    y_preds = []\n",
        "    print()\n",
        "    print(model_name2)\n",
        "    print()\n",
        "\n",
        "    model2 = create_model(model_name2)\n",
        "\n",
        "    # Compile the model\n",
        "    model2.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0001),\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "    # Generate a print\n",
        "\n",
        "    # Fit data to model\n",
        "    history2 = model2.fit(x=train,\n",
        "                          validation_data=validation,\n",
        "                          epochs=NUM_EPOCHS\n",
        "\n",
        "                          )\n",
        "\n",
        "    # model save..\n",
        "    model_saved_name = model_name2 + \"_weights\" + \"_\" + str(fold_no) + \".h5\"\n",
        "\n",
        "    model2.save(\"require path\" + model_saved_name)\n",
        "\n",
        "    hist_df = pd.DataFrame(history2.history) \n",
        "    hist_csv_file =  \"history_\" + model_name2 + \"_weights\" + \"_\" + str(fold_no) + \".csv\"\n",
        "    filepath = \"require path\" + hist_csv_file \n",
        "    with open(filepath, mode='w') as f:\n",
        "        hist_df.to_csv(f)\n",
        "\n",
        "    print(f'{model_saved_name} saved')\n",
        "    print(f'{hist_csv_file} saved')\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores = model2.evaluate(validation)\n",
        "    print(\n",
        "        f'Score for fold {fold_no}: {model2.metrics_names[0]} of {scores[0]}; {model2.metrics_names[1]} of {scores[1] * 100}%')\n",
        "    # predictions = model.predict()\n",
        "    preds2 = model2.predict(test, batch_size=validation_batch)\n",
        "    for pred in preds2:\n",
        "        y_preds.append(np.argmax(pred))\n",
        "\n",
        "    print('Accuracy Score: ', accuracy_score(y_val, y_preds))\n",
        "\n",
        "    print('Precision Score(Class wise): ', precision_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(precision_score(y_val, y_preds, average=None)) / n)\n",
        "    print('Recall Score(Class wise): ', recall_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(recall_score(y_val, y_preds, average=None)) / n)\n",
        "    print('F1 Score(Class wise): ', f1_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(f1_score(y_val, y_preds, average=None)) / n)\n",
        "    print('Conf Matrix Score(Class wise):\\n ', confusion_matrix(y_val, y_preds))\n",
        "\n",
        "    y_preds = []\n",
        "    print()\n",
        "    print(model_name3)\n",
        "    print()\n",
        "\n",
        "    model3 = create_model(model_name3)\n",
        "\n",
        "    # Compile the model\n",
        "    model3.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0001),\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "    # Generate a print\n",
        "\n",
        "    # Fit data to model\n",
        "    history3 = model3.fit(x=train,\n",
        "                          validation_data=validation,\n",
        "                          epochs=NUM_EPOCHS\n",
        "\n",
        "                          )\n",
        "\n",
        "    # model save..\n",
        "    model_saved_name = model_name3 + \"_weights\" + \"_\" + str(fold_no) + \".h5\"\n",
        "\n",
        "    model3.save(\"require path\" + model_saved_name)\n",
        "\n",
        "    hist_df = pd.DataFrame(history2.history)\n",
        "    hist_csv_file =  \"history_\" + model_name3 + \"_weights\" + \"_\" + str(fold_no) + \".csv\"\n",
        "    filepath = \"require path\" + hist_csv_file \n",
        "    with open(filepath, mode='w') as f:\n",
        "        hist_df.to_csv(f)\n",
        "\n",
        "    print(f'{model_saved_name} saved')\n",
        "    print(f'{hist_csv_file} saved')\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores = model3.evaluate(validation)\n",
        "    print(\n",
        "        f'Score for fold {fold_no}: {model3.metrics_names[0]} of {scores[0]}; {model3.metrics_names[1]} of {scores[1] * 100}%')\n",
        "    # predictions = model.predict()\n",
        "    preds3 = model3.predict(test, batch_size=validation_batch)\n",
        "    for pred in preds3:\n",
        "        y_preds.append(np.argmax(pred))\n",
        "\n",
        "    print('Accuracy Score: ', accuracy_score(y_val, y_preds))\n",
        "\n",
        "    print('Precision Score(Class wise): ', precision_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(precision_score(y_val, y_preds, average=None)) / n)\n",
        "    print('Recall Score(Class wise): ', recall_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(recall_score(y_val, y_preds, average=None)) / n)\n",
        "    print('F1 Score(Class wise): ', f1_score(y_val, y_preds, average=None), \" mean- \",\n",
        "          sum(f1_score(y_val, y_preds, average=None)) / n)\n",
        "    print('Conf Matrix Score(Class wise):\\n ', confusion_matrix(y_val, y_preds))\n",
        "\n",
        "    ensem_pred = fuzzy_dist(preds1, preds2, preds3)\n",
        "    print('Post Ensemble Accuracy Score: ', accuracy_score(y_val, ensem_pred))\n",
        "\n",
        "    print('Post Ensemble Precision Score(Class wise): ', precision_score(y_val, ensem_pred, average=None), \" mean- \",\n",
        "          sum(precision_score(y_val, ensem_pred, average=None)) / n)\n",
        "    print('Post Ensemble Recall Score(Class wise): ', recall_score(y_val, ensem_pred, average=None), \" mean- \",\n",
        "          sum(recall_score(y_val, ensem_pred, average=None)) / n)\n",
        "    print('Post Ensemble F1 Score(Class wise): ', f1_score(y_val, ensem_pred, average=None), \" mean- \",\n",
        "          sum(f1_score(y_val, ensem_pred, average=None)) / n)\n",
        "    print('Post Ensemble Conf Matrix Score(Class wise):\\n ', confusion_matrix(y_val, ensem_pred))"
      ],
      "metadata": {
        "id": "XsMihxc3v3Nz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = generate_csv('/content/drive/MyDrive/Colab Notebooks/CSC413/final')\n",
        "\n",
        "y = np.array(list(df[\"class\"]))\n",
        "x = np.array(list(df[\"path\"]))\n",
        "\n",
        "files_for_train_x = []\n",
        "files_for_validation_x = []\n",
        "files_for_train_y = []\n",
        "files_for_validation_y = []\n",
        "\n",
        "k_fold_splits(x, y, files_for_train_x, files_for_validation_x,\n",
        "              files_for_train_y, files_for_validation_y, n_splits=5)  # n_splits = 5\n",
        "\n",
        "\n",
        "# N is the number of folds\n",
        "N = len(files_for_train_x)\n",
        "for i in range(0, N):\n",
        "    k_fold_separate(files_for_train_x[i], files_for_train_y[i],\n",
        "                    files_for_validation_x[i], files_for_validation_y[i],\n",
        "                    \"InceptionV3\", \"MobileNetV2\", \"InceptionResNetV2\", i + 1,\n",
        "                    NUM_EPOCHS=70, train_batch=16,\n",
        "                    validation_batch=16, lr=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PoMki3Qv4X2",
        "outputId": "f38bf8e1-d861-467b-fcbf-de9185283b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV being generated\n",
            "Generation Complete\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "fold no ---  1\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "InceptionV3\n",
            "\n",
            "Epoch 1/70\n",
            "203/203 [==============================] - 50s 213ms/step - loss: 1.1013 - accuracy: 0.5881 - val_loss: 2.7818 - val_accuracy: 0.1790\n",
            "Epoch 2/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.8045 - accuracy: 0.7123 - val_loss: 3.9644 - val_accuracy: 0.2802\n",
            "Epoch 3/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.7461 - accuracy: 0.7357 - val_loss: 0.7811 - val_accuracy: 0.7123\n",
            "Epoch 4/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.7142 - accuracy: 0.7487 - val_loss: 0.6824 - val_accuracy: 0.7519\n",
            "Epoch 5/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.6766 - accuracy: 0.7552 - val_loss: 0.5625 - val_accuracy: 0.7975\n",
            "Epoch 6/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.6554 - accuracy: 0.7644 - val_loss: 0.5439 - val_accuracy: 0.7975\n",
            "Epoch 7/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.6025 - accuracy: 0.7799 - val_loss: 0.7493 - val_accuracy: 0.7259\n",
            "Epoch 8/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.5896 - accuracy: 0.7919 - val_loss: 0.4822 - val_accuracy: 0.8296\n",
            "Epoch 9/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.5713 - accuracy: 0.7953 - val_loss: 0.4978 - val_accuracy: 0.8099\n",
            "Epoch 10/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.5338 - accuracy: 0.8073 - val_loss: 0.3995 - val_accuracy: 0.8568\n",
            "Epoch 11/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.5277 - accuracy: 0.8151 - val_loss: 0.4301 - val_accuracy: 0.8556\n",
            "Epoch 12/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.5055 - accuracy: 0.8215 - val_loss: 0.4150 - val_accuracy: 0.8568\n",
            "Epoch 13/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.5100 - accuracy: 0.8151 - val_loss: 0.5561 - val_accuracy: 0.8185\n",
            "Epoch 14/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.4925 - accuracy: 0.8206 - val_loss: 0.4300 - val_accuracy: 0.8506\n",
            "Epoch 15/70\n",
            "203/203 [==============================] - 43s 210ms/step - loss: 0.4632 - accuracy: 0.8320 - val_loss: 0.3585 - val_accuracy: 0.8765\n",
            "Epoch 16/70\n",
            "203/203 [==============================] - 43s 209ms/step - loss: 0.4754 - accuracy: 0.8308 - val_loss: 0.3657 - val_accuracy: 0.8790\n",
            "Epoch 17/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.4028 - accuracy: 0.8552 - val_loss: 0.3630 - val_accuracy: 0.8765\n",
            "Epoch 18/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.4125 - accuracy: 0.8540 - val_loss: 0.3890 - val_accuracy: 0.8568\n",
            "Epoch 19/70\n",
            "203/203 [==============================] - 43s 209ms/step - loss: 0.4207 - accuracy: 0.8537 - val_loss: 0.3556 - val_accuracy: 0.8852\n",
            "Epoch 20/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.4187 - accuracy: 0.8543 - val_loss: 0.3098 - val_accuracy: 0.8963\n",
            "Epoch 21/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.4092 - accuracy: 0.8549 - val_loss: 0.2898 - val_accuracy: 0.9111\n",
            "Epoch 22/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.3675 - accuracy: 0.8750 - val_loss: 0.3049 - val_accuracy: 0.9062\n",
            "Epoch 23/70\n",
            "203/203 [==============================] - 43s 211ms/step - loss: 0.3819 - accuracy: 0.8669 - val_loss: 0.3408 - val_accuracy: 0.8667\n",
            "Epoch 24/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.3628 - accuracy: 0.8709 - val_loss: 0.3396 - val_accuracy: 0.8840\n",
            "Epoch 25/70\n",
            "203/203 [==============================] - 43s 211ms/step - loss: 0.3733 - accuracy: 0.8676 - val_loss: 0.2681 - val_accuracy: 0.9185\n",
            "Epoch 26/70\n",
            "203/203 [==============================] - 43s 210ms/step - loss: 0.3360 - accuracy: 0.8787 - val_loss: 0.4465 - val_accuracy: 0.8395\n",
            "Epoch 27/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.3502 - accuracy: 0.8858 - val_loss: 0.3728 - val_accuracy: 0.8901\n",
            "Epoch 28/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.3268 - accuracy: 0.8839 - val_loss: 0.3251 - val_accuracy: 0.8852\n",
            "Epoch 29/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.3107 - accuracy: 0.8916 - val_loss: 0.2462 - val_accuracy: 0.9185\n",
            "Epoch 30/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.3183 - accuracy: 0.8821 - val_loss: 0.3327 - val_accuracy: 0.8951\n",
            "Epoch 31/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.3101 - accuracy: 0.8944 - val_loss: 0.2382 - val_accuracy: 0.9235\n",
            "Epoch 32/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2975 - accuracy: 0.8956 - val_loss: 0.3063 - val_accuracy: 0.8951\n",
            "Epoch 33/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.3031 - accuracy: 0.8919 - val_loss: 0.2505 - val_accuracy: 0.9136\n",
            "Epoch 34/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.2865 - accuracy: 0.8981 - val_loss: 0.2419 - val_accuracy: 0.9210\n",
            "Epoch 35/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2919 - accuracy: 0.9003 - val_loss: 0.2309 - val_accuracy: 0.9210\n",
            "Epoch 36/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2701 - accuracy: 0.9086 - val_loss: 0.2596 - val_accuracy: 0.9198\n",
            "Epoch 37/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.2799 - accuracy: 0.9006 - val_loss: 0.2512 - val_accuracy: 0.9074\n",
            "Epoch 38/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2658 - accuracy: 0.9071 - val_loss: 0.2509 - val_accuracy: 0.9198\n",
            "Epoch 39/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2627 - accuracy: 0.9043 - val_loss: 0.2435 - val_accuracy: 0.9247\n",
            "Epoch 40/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2575 - accuracy: 0.9080 - val_loss: 0.2744 - val_accuracy: 0.9111\n",
            "Epoch 41/70\n",
            "203/203 [==============================] - 43s 209ms/step - loss: 0.2546 - accuracy: 0.9074 - val_loss: 0.2232 - val_accuracy: 0.9395\n",
            "Epoch 42/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.2582 - accuracy: 0.9049 - val_loss: 0.2311 - val_accuracy: 0.9160\n",
            "Epoch 43/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.2612 - accuracy: 0.9046 - val_loss: 0.2395 - val_accuracy: 0.9185\n",
            "Epoch 44/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2364 - accuracy: 0.9173 - val_loss: 0.2011 - val_accuracy: 0.9395\n",
            "Epoch 45/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2437 - accuracy: 0.9142 - val_loss: 0.2607 - val_accuracy: 0.9086\n",
            "Epoch 46/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.2179 - accuracy: 0.9250 - val_loss: 0.2091 - val_accuracy: 0.9259\n",
            "Epoch 47/70\n",
            "203/203 [==============================] - 43s 211ms/step - loss: 0.2217 - accuracy: 0.9244 - val_loss: 0.2693 - val_accuracy: 0.8988\n",
            "Epoch 48/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.2303 - accuracy: 0.9145 - val_loss: 0.2315 - val_accuracy: 0.9185\n",
            "Epoch 49/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.2324 - accuracy: 0.9139 - val_loss: 0.2071 - val_accuracy: 0.9296\n",
            "Epoch 50/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.2123 - accuracy: 0.9296 - val_loss: 0.1950 - val_accuracy: 0.9383\n",
            "Epoch 51/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2148 - accuracy: 0.9222 - val_loss: 0.2624 - val_accuracy: 0.9111\n",
            "Epoch 52/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.2265 - accuracy: 0.9219 - val_loss: 0.1950 - val_accuracy: 0.9370\n",
            "Epoch 53/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.1913 - accuracy: 0.9305 - val_loss: 0.2694 - val_accuracy: 0.9210\n",
            "Epoch 54/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2021 - accuracy: 0.9290 - val_loss: 0.2053 - val_accuracy: 0.9333\n",
            "Epoch 55/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.1936 - accuracy: 0.9287 - val_loss: 0.1943 - val_accuracy: 0.9395\n",
            "Epoch 56/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.1941 - accuracy: 0.9287 - val_loss: 0.1643 - val_accuracy: 0.9494\n",
            "Epoch 57/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.2014 - accuracy: 0.9250 - val_loss: 0.2038 - val_accuracy: 0.9383\n",
            "Epoch 58/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.2070 - accuracy: 0.9250 - val_loss: 0.1486 - val_accuracy: 0.9543\n",
            "Epoch 59/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.1760 - accuracy: 0.9352 - val_loss: 0.1865 - val_accuracy: 0.9383\n",
            "Epoch 60/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.1845 - accuracy: 0.9349 - val_loss: 0.2546 - val_accuracy: 0.9272\n",
            "Epoch 61/70\n",
            "203/203 [==============================] - 43s 209ms/step - loss: 0.1653 - accuracy: 0.9349 - val_loss: 0.1494 - val_accuracy: 0.9494\n",
            "Epoch 62/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.1615 - accuracy: 0.9447 - val_loss: 0.1951 - val_accuracy: 0.9420\n",
            "Epoch 63/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.1561 - accuracy: 0.9410 - val_loss: 0.2008 - val_accuracy: 0.9309\n",
            "Epoch 64/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.1632 - accuracy: 0.9429 - val_loss: 0.1708 - val_accuracy: 0.9457\n",
            "Epoch 65/70\n",
            "203/203 [==============================] - 43s 210ms/step - loss: 0.1719 - accuracy: 0.9389 - val_loss: 0.1637 - val_accuracy: 0.9432\n",
            "Epoch 66/70\n",
            "203/203 [==============================] - 43s 211ms/step - loss: 0.1646 - accuracy: 0.9407 - val_loss: 0.1598 - val_accuracy: 0.9494\n",
            "Epoch 67/70\n",
            "203/203 [==============================] - 42s 209ms/step - loss: 0.1650 - accuracy: 0.9435 - val_loss: 0.1816 - val_accuracy: 0.9333\n",
            "Epoch 68/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.1558 - accuracy: 0.9463 - val_loss: 0.1915 - val_accuracy: 0.9395\n",
            "Epoch 69/70\n",
            "203/203 [==============================] - 42s 207ms/step - loss: 0.1305 - accuracy: 0.9537 - val_loss: 0.1641 - val_accuracy: 0.9506\n",
            "Epoch 70/70\n",
            "203/203 [==============================] - 42s 208ms/step - loss: 0.1510 - accuracy: 0.9472 - val_loss: 0.1691 - val_accuracy: 0.9469\n",
            "InceptionV3_weights_1.h5 saved\n",
            "history_InceptionV3_weights_1.csv saved\n",
            "51/51 [==============================] - 2s 38ms/step - loss: 0.1691 - accuracy: 0.9469\n",
            "Score for fold 1: loss of 0.1690772920846939; accuracy of 94.69135999679565%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score:  0.9469135802469136\n",
            "Precision Score(Class wise):  [0.95705521 0.92810458 0.9010989  0.99310345 0.96407186]  mean-  0.9486867991099027\n",
            "Recall Score(Class wise):  [0.95121951 0.84023669 0.97619048 0.99310345 0.98170732]  mean-  0.9484914880250326\n",
            "F1 Score(Class wise):  [0.95412844 0.88198758 0.93714286 0.99310345 0.97280967]  mean-  0.9478343982198318\n",
            "Conf Matrix Score(Class wise):\n",
            "  [[156   8   0   0   0]\n",
            " [  6 142  17   0   4]\n",
            " [  1   1 164   0   2]\n",
            " [  0   0   1 144   0]\n",
            " [  0   2   0   1 161]]\n",
            "\n",
            "MobileNetV2\n",
            "\n",
            "Epoch 1/70\n",
            "203/203 [==============================] - 47s 207ms/step - loss: 1.3641 - accuracy: 0.4094 - val_loss: 1.6136 - val_accuracy: 0.1790\n",
            "Epoch 2/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 1.0025 - accuracy: 0.5980 - val_loss: 1.6214 - val_accuracy: 0.1790\n",
            "Epoch 3/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.8694 - accuracy: 0.6635 - val_loss: 1.6251 - val_accuracy: 0.1790\n",
            "Epoch 4/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.7673 - accuracy: 0.7058 - val_loss: 1.6335 - val_accuracy: 0.1790\n",
            "Epoch 5/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.7033 - accuracy: 0.7336 - val_loss: 1.6479 - val_accuracy: 0.1790\n",
            "Epoch 6/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.6745 - accuracy: 0.7465 - val_loss: 1.6785 - val_accuracy: 0.1790\n",
            "Epoch 7/70\n",
            "203/203 [==============================] - 42s 205ms/step - loss: 0.6460 - accuracy: 0.7515 - val_loss: 1.6733 - val_accuracy: 0.1790\n",
            "Epoch 8/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.6376 - accuracy: 0.7684 - val_loss: 1.7059 - val_accuracy: 0.1790\n",
            "Epoch 9/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.6164 - accuracy: 0.7746 - val_loss: 1.7482 - val_accuracy: 0.1790\n",
            "Epoch 10/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.5972 - accuracy: 0.7870 - val_loss: 1.7946 - val_accuracy: 0.1790\n",
            "Epoch 11/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.5703 - accuracy: 0.7891 - val_loss: 1.8388 - val_accuracy: 0.1790\n",
            "Epoch 12/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.5394 - accuracy: 0.8021 - val_loss: 1.8333 - val_accuracy: 0.1790\n",
            "Epoch 13/70\n",
            "203/203 [==============================] - 41s 201ms/step - loss: 0.5123 - accuracy: 0.8104 - val_loss: 1.8198 - val_accuracy: 0.1790\n",
            "Epoch 14/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.4932 - accuracy: 0.8175 - val_loss: 1.8136 - val_accuracy: 0.1790\n",
            "Epoch 15/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.4945 - accuracy: 0.8172 - val_loss: 1.8672 - val_accuracy: 0.1790\n",
            "Epoch 16/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.4832 - accuracy: 0.8138 - val_loss: 1.9683 - val_accuracy: 0.1790\n",
            "Epoch 17/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.4798 - accuracy: 0.8237 - val_loss: 1.8774 - val_accuracy: 0.1790\n",
            "Epoch 18/70\n",
            "203/203 [==============================] - 42s 204ms/step - loss: 0.4603 - accuracy: 0.8274 - val_loss: 2.0107 - val_accuracy: 0.1790\n",
            "Epoch 19/70\n",
            "203/203 [==============================] - 42s 204ms/step - loss: 0.4325 - accuracy: 0.8429 - val_loss: 2.2067 - val_accuracy: 0.1790\n",
            "Epoch 20/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.4237 - accuracy: 0.8419 - val_loss: 2.0821 - val_accuracy: 0.1790\n",
            "Epoch 21/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.4091 - accuracy: 0.8524 - val_loss: 2.2221 - val_accuracy: 0.1790\n",
            "Epoch 22/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.4136 - accuracy: 0.8472 - val_loss: 2.5375 - val_accuracy: 0.1790\n",
            "Epoch 23/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.3826 - accuracy: 0.8577 - val_loss: 2.6152 - val_accuracy: 0.1790\n",
            "Epoch 24/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.3714 - accuracy: 0.8635 - val_loss: 2.5744 - val_accuracy: 0.1790\n",
            "Epoch 25/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.3932 - accuracy: 0.8571 - val_loss: 3.0658 - val_accuracy: 0.1790\n",
            "Epoch 26/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.3726 - accuracy: 0.8592 - val_loss: 2.5186 - val_accuracy: 0.1790\n",
            "Epoch 27/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.3637 - accuracy: 0.8700 - val_loss: 2.6309 - val_accuracy: 0.1790\n",
            "Epoch 28/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.3653 - accuracy: 0.8651 - val_loss: 2.5095 - val_accuracy: 0.1790\n",
            "Epoch 29/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.3342 - accuracy: 0.8780 - val_loss: 2.9012 - val_accuracy: 0.1790\n",
            "Epoch 30/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.3523 - accuracy: 0.8706 - val_loss: 2.7625 - val_accuracy: 0.1790\n",
            "Epoch 31/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.3366 - accuracy: 0.8706 - val_loss: 2.9949 - val_accuracy: 0.1790\n",
            "Epoch 32/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.3317 - accuracy: 0.8768 - val_loss: 3.0790 - val_accuracy: 0.3160\n",
            "Epoch 33/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.3175 - accuracy: 0.8858 - val_loss: 2.7356 - val_accuracy: 0.3444\n",
            "Epoch 34/70\n",
            "203/203 [==============================] - 41s 201ms/step - loss: 0.3304 - accuracy: 0.8836 - val_loss: 2.5212 - val_accuracy: 0.3086\n",
            "Epoch 35/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.3073 - accuracy: 0.8901 - val_loss: 2.6125 - val_accuracy: 0.2889\n",
            "Epoch 36/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.3088 - accuracy: 0.8845 - val_loss: 1.3871 - val_accuracy: 0.5432\n",
            "Epoch 37/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2936 - accuracy: 0.8981 - val_loss: 0.9773 - val_accuracy: 0.6481\n",
            "Epoch 38/70\n",
            "203/203 [==============================] - 41s 201ms/step - loss: 0.3073 - accuracy: 0.8858 - val_loss: 0.7358 - val_accuracy: 0.7457\n",
            "Epoch 39/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2981 - accuracy: 0.8910 - val_loss: 0.6852 - val_accuracy: 0.7506\n",
            "Epoch 40/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2765 - accuracy: 0.8987 - val_loss: 0.4998 - val_accuracy: 0.8222\n",
            "Epoch 41/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2825 - accuracy: 0.8972 - val_loss: 0.3429 - val_accuracy: 0.8765\n",
            "Epoch 42/70\n",
            "203/203 [==============================] - 41s 201ms/step - loss: 0.2729 - accuracy: 0.9012 - val_loss: 0.3394 - val_accuracy: 0.8704\n",
            "Epoch 43/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2850 - accuracy: 0.8990 - val_loss: 0.4119 - val_accuracy: 0.8593\n",
            "Epoch 44/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2777 - accuracy: 0.9003 - val_loss: 0.4185 - val_accuracy: 0.8519\n",
            "Epoch 45/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2430 - accuracy: 0.9132 - val_loss: 0.4449 - val_accuracy: 0.8481\n",
            "Epoch 46/70\n",
            "203/203 [==============================] - 41s 201ms/step - loss: 0.2633 - accuracy: 0.9046 - val_loss: 0.3426 - val_accuracy: 0.8691\n",
            "Epoch 47/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2553 - accuracy: 0.9046 - val_loss: 0.2851 - val_accuracy: 0.9062\n",
            "Epoch 48/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2445 - accuracy: 0.9086 - val_loss: 0.3092 - val_accuracy: 0.8951\n",
            "Epoch 49/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2450 - accuracy: 0.9055 - val_loss: 0.3201 - val_accuracy: 0.9000\n",
            "Epoch 50/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2431 - accuracy: 0.9126 - val_loss: 0.3222 - val_accuracy: 0.9148\n",
            "Epoch 51/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2405 - accuracy: 0.9139 - val_loss: 0.5849 - val_accuracy: 0.8198\n",
            "Epoch 52/70\n",
            "203/203 [==============================] - 42s 206ms/step - loss: 0.2305 - accuracy: 0.9191 - val_loss: 0.3672 - val_accuracy: 0.8753\n",
            "Epoch 53/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2313 - accuracy: 0.9160 - val_loss: 0.3370 - val_accuracy: 0.8852\n",
            "Epoch 54/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2337 - accuracy: 0.9191 - val_loss: 0.3967 - val_accuracy: 0.8654\n",
            "Epoch 55/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2242 - accuracy: 0.9237 - val_loss: 0.4287 - val_accuracy: 0.8580\n",
            "Epoch 56/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2325 - accuracy: 0.9129 - val_loss: 0.3654 - val_accuracy: 0.8802\n",
            "Epoch 57/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2101 - accuracy: 0.9250 - val_loss: 0.3074 - val_accuracy: 0.8901\n",
            "Epoch 58/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2210 - accuracy: 0.9188 - val_loss: 0.3261 - val_accuracy: 0.8815\n",
            "Epoch 59/70\n",
            "203/203 [==============================] - 41s 204ms/step - loss: 0.2214 - accuracy: 0.9207 - val_loss: 0.2957 - val_accuracy: 0.8963\n",
            "Epoch 60/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2095 - accuracy: 0.9299 - val_loss: 0.3465 - val_accuracy: 0.8840\n",
            "Epoch 61/70\n",
            "203/203 [==============================] - 41s 204ms/step - loss: 0.2070 - accuracy: 0.9225 - val_loss: 0.3012 - val_accuracy: 0.9025\n",
            "Epoch 62/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2130 - accuracy: 0.9228 - val_loss: 0.3695 - val_accuracy: 0.8617\n",
            "Epoch 63/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2069 - accuracy: 0.9250 - val_loss: 0.3357 - val_accuracy: 0.8852\n",
            "Epoch 64/70\n",
            "203/203 [==============================] - 42s 204ms/step - loss: 0.1976 - accuracy: 0.9324 - val_loss: 0.2554 - val_accuracy: 0.9160\n",
            "Epoch 65/70\n",
            "203/203 [==============================] - 41s 203ms/step - loss: 0.2072 - accuracy: 0.9268 - val_loss: 0.3153 - val_accuracy: 0.8914\n",
            "Epoch 66/70\n",
            "203/203 [==============================] - 42s 205ms/step - loss: 0.2029 - accuracy: 0.9305 - val_loss: 0.3675 - val_accuracy: 0.8827\n",
            "Epoch 67/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.2070 - accuracy: 0.9234 - val_loss: 0.2637 - val_accuracy: 0.9123\n",
            "Epoch 68/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.1902 - accuracy: 0.9274 - val_loss: 0.2720 - val_accuracy: 0.9099\n",
            "Epoch 69/70\n",
            "203/203 [==============================] - 41s 202ms/step - loss: 0.1739 - accuracy: 0.9333 - val_loss: 0.2546 - val_accuracy: 0.9272\n",
            "Epoch 70/70\n",
            "203/203 [==============================] - 41s 201ms/step - loss: 0.1993 - accuracy: 0.9302 - val_loss: 0.2370 - val_accuracy: 0.9235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNetV2_weights_1.h5 saved\n",
            "history_MobileNetV2_weights_1.csv saved\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.2370 - accuracy: 0.9235\n",
            "Score for fold 1: loss of 0.23699422180652618; accuracy of 92.34567880630493%\n",
            "Accuracy Score:  0.9234567901234568\n",
            "Precision Score(Class wise):  [0.92485549 0.92307692 0.86740331 0.97902098 0.93529412]  mean-  0.9259301651983136\n",
            "Recall Score(Class wise):  [0.97560976 0.78106509 0.93452381 0.96551724 0.9695122 ]  mean-  0.9252456181760058\n",
            "F1 Score(Class wise):  [0.9495549  0.84615385 0.89971347 0.97222222 0.95209581]  mean-  0.9239480479900891\n",
            "Conf Matrix Score(Class wise):\n",
            "  [[160   3   1   0   0]\n",
            " [ 13 132  18   0   6]\n",
            " [  0   5 157   3   3]\n",
            " [  0   0   3 140   2]\n",
            " [  0   3   2   0 159]]\n",
            "\n",
            "InceptionResNetV2\n",
            "\n",
            "Epoch 1/70\n",
            "203/203 [==============================] - 78s 298ms/step - loss: 0.7656 - accuracy: 0.7206 - val_loss: 3.6785 - val_accuracy: 0.1790\n",
            "Epoch 2/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.5865 - accuracy: 0.7864 - val_loss: 1.8155 - val_accuracy: 0.3630\n",
            "Epoch 3/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.5051 - accuracy: 0.8166 - val_loss: 0.5231 - val_accuracy: 0.8049\n",
            "Epoch 4/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.4478 - accuracy: 0.8435 - val_loss: 0.3428 - val_accuracy: 0.8802\n",
            "Epoch 5/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.4465 - accuracy: 0.8444 - val_loss: 0.3123 - val_accuracy: 0.8877\n",
            "Epoch 6/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.3763 - accuracy: 0.8642 - val_loss: 0.4543 - val_accuracy: 0.8667\n",
            "Epoch 7/70\n",
            "203/203 [==============================] - 57s 281ms/step - loss: 0.3757 - accuracy: 0.8700 - val_loss: 0.2857 - val_accuracy: 0.9012\n",
            "Epoch 8/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.3489 - accuracy: 0.8694 - val_loss: 0.2812 - val_accuracy: 0.9086\n",
            "Epoch 9/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.3539 - accuracy: 0.8734 - val_loss: 0.2757 - val_accuracy: 0.9025\n",
            "Epoch 10/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.3235 - accuracy: 0.8836 - val_loss: 0.2752 - val_accuracy: 0.9037\n",
            "Epoch 11/70\n",
            "203/203 [==============================] - 57s 278ms/step - loss: 0.2939 - accuracy: 0.9021 - val_loss: 0.2167 - val_accuracy: 0.9247\n",
            "Epoch 12/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2888 - accuracy: 0.8910 - val_loss: 0.2164 - val_accuracy: 0.9259\n",
            "Epoch 13/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.2796 - accuracy: 0.8975 - val_loss: 0.2370 - val_accuracy: 0.9309\n",
            "Epoch 14/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2890 - accuracy: 0.8987 - val_loss: 0.2456 - val_accuracy: 0.9173\n",
            "Epoch 15/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2680 - accuracy: 0.9068 - val_loss: 0.2404 - val_accuracy: 0.9173\n",
            "Epoch 16/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.2782 - accuracy: 0.9015 - val_loss: 0.2606 - val_accuracy: 0.9099\n",
            "Epoch 17/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2633 - accuracy: 0.9009 - val_loss: 0.2198 - val_accuracy: 0.9235\n",
            "Epoch 18/70\n",
            "203/203 [==============================] - 57s 281ms/step - loss: 0.2660 - accuracy: 0.9043 - val_loss: 0.2846 - val_accuracy: 0.9074\n",
            "Epoch 19/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.2619 - accuracy: 0.9003 - val_loss: 0.2235 - val_accuracy: 0.9309\n",
            "Epoch 20/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2686 - accuracy: 0.9055 - val_loss: 0.2024 - val_accuracy: 0.9395\n",
            "Epoch 21/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.2127 - accuracy: 0.9247 - val_loss: 0.1947 - val_accuracy: 0.9395\n",
            "Epoch 22/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2220 - accuracy: 0.9234 - val_loss: 0.2167 - val_accuracy: 0.9321\n",
            "Epoch 23/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.2149 - accuracy: 0.9200 - val_loss: 0.2264 - val_accuracy: 0.9198\n",
            "Epoch 24/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2354 - accuracy: 0.9191 - val_loss: 0.1973 - val_accuracy: 0.9346\n",
            "Epoch 25/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.2137 - accuracy: 0.9185 - val_loss: 0.2136 - val_accuracy: 0.9395\n",
            "Epoch 26/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.2033 - accuracy: 0.9271 - val_loss: 0.1763 - val_accuracy: 0.9432\n",
            "Epoch 27/70\n",
            "203/203 [==============================] - 57s 280ms/step - loss: 0.1767 - accuracy: 0.9364 - val_loss: 0.2405 - val_accuracy: 0.9247\n",
            "Epoch 28/70\n",
            "203/203 [==============================] - 57s 279ms/step - loss: 0.1910 - accuracy: 0.9278 - val_loss: 0.2925 - val_accuracy: 0.9123\n",
            "Epoch 29/70\n",
            " 73/203 [=========>....................] - ETA: 33s - loss: 0.2026 - accuracy: 0.9358"
          ]
        }
      ]
    }
  ]
}